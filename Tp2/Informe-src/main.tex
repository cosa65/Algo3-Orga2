\documentclass[a4paper]{article}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{charter}   % tipografia
\usepackage{graphicx}
\usepackage{courier}
%\usepackage{makeidx}
\usepackage{paralist} %itemize inline

%\usepackage{float}
\usepackage{amsmath, amsthm, amssymb}
%\usepackage{amsfonts}
%\usepackage{sectsty}
%\usepackage{charter}
%\usepackage{wrapfig}
%\usepackage{listings}
%\lstset{language=C}


\input{page.layout}
% \setcounter{secnumdepth}{2}
\usepackage{underscore}
\usepackage{caratula}
\usepackage{url}


\begin{document}


\thispagestyle{empty}
\materia{Métodos Numéricos}
\submateria{Segundo Cuatrimestre de 2014}
\titulo{Trabajo Práctico 2}
\subtitulo{Tirate un qu\'e, tirate un \emph{ranking}...}
\integrante{Fosco, Martin Esteban}{449/13}{mfosco65@gmail.com}
\integrante{Minces Müller, Javier Nicolás}{231/13}{javijavi1994@gmail.com}
\integrante{Chibana, Christian Ezequiel}{586/13}{christian.chiba93@gmail.com}

\maketitle
\newpage

\thispagestyle{empty}
\vfill
\begin{abstract}
En este trabajo se analizan tres métodos para crear un $ranking$ de páginas web: PageRank, HITS e In-Deg. Estos algoritmos utilizan matrices para modelar la Web mediante matrices y utilizan métodos numéricos, como el método de la potencia, para ordenar las páginas a partir de los links que las relacionan. %método, método, método
\end{abstract}

\thispagestyle{empty}
\vspace{3cm}
\tableofcontents
\newpage


%\normalsize
\newpage

\section{Introducci\'{o}n Te\'{o}rica}
\label{sec:intro}
%Contendrá una breve explicación de la base teórica que fundamenta los métodos involucrados en el trabajo, junto con los métodos mismos. No deben incluirse demostracionese de propiedades ni teoremas, ejemplos innecesarios, ni definiciones elementales (como por ejemplo la de matriz simétrica).

El problema a resolver es encontrar una manera confiable de definir en qué páginas es conveniente comprar espacios de propaganda para garantizar la mayor publicidad posible a un grupo de música.
El objetivo de este trabajo, por tanto, es resolver este problema analizado diferentes métodos que nos permitan definir un ranking de importancia de p\'{a}ginas web. \\

En un primer paso se debe acordar un m\'{e}todo de selecci\'{o}n de las p\'{a}ginas web sobre las cuales se trabaja, es decir que es necesario adquirir una subred de p\'{a}ginas relevantes al tema en cuesti\'{o}n

Se recurre en primer lugar a un buscador textual, pero este no asegura un resultado confiable en cuanto a la relevancia de las páginas recibidas. En el paper escrito por Kleinberg \footnote{Kleinberg - 1999 - \textit{Authoritative sources in a hyperlinked environment}} se sugiere un conjunto inicial de páginas $S_{\sigma}$ que cumple las siguientes condiciones:

\begin{itemize}

\item $S_{\sigma}$ es relativamente pequeña.
\item $S_{\sigma}$ contiene una gran cantidad de p\'{a}ginas relevantes.
\item $S_{\sigma}$ contiene muchas de las autoridades de m\'{a}s peso.

\end{itemize}

El autor propone una forma de ampliar este conjunto inicial para asegurar que se cumpla la tercera condición: buscar autoridades de gran peso a las que haya links en la subred obtenida del buscador e incluir \'{e}stas tambien en $S_{\sigma}$. 

Se construye luego un mapa de este conjunto de páginas en forma de grafo dirigido, observando los links entre ellas. Hasta este punto, el proceso escapa al objetivo del trabajo, que considera dado este mapa.\\ \\

Terminado este paso, se procede a construir la matriz de conectividad $W \in n \times n$, tal que $w_{ij}=1$ si existe un link de la página $j$ a la página $i$ y $w_{ij}=0$ en caso contrario. 

Usando esta matriz (con la misma notación), se pueden aplicar distintos m\'{e}todos para procesar y definir un cierto \textit{ranking} de las p\'{a}ginas.
Esta matriz es esparsa, porque incluso en redes de varios cientos de miles de páginas, en promedio cada una tiene 7 u 8 links salientes. \footnote{Kamvar}
Esto debe ser tenido en cuenta a la hora de guardar la matriz en memoria.

Los métodos a analizar son: \\


\textbf{PageRank}, el más conocido de los utilizados por Google. PageRank asigna un puntaje a cada página según el puntaje de las que apuntan a ella. De esta forma se evita que una página con muchos links de páginas sin valor obtenga un puntaje demasiado alto. Además, modifica W para tener en cuenta que un "navegante aleatorio" puede con una determinada probabilidad saltar a cualquier página de la web, algo que también hace si encuentra una página sin links salientes.

Esto se traduce como buscar un vector $x$ tal que $Wx=x$, es decir, encontrar el autovector para el autovalor 1. Si se sabe que 1 es el autovalor de mayor módulo, se puede encontrar su autovector con el método de las potencias.\\

\textbf{HITS} (Hyperlink-Induced Topic Search), este se basa en la existencia de páginas de gran valor como 'autoridad' y como "hub". En cada iteración, se actualiza el puntaje de cada página como autoridad a partir del puntaje hub de las páginas que la citan. Luego, se actualiza el puntaje como hub de cada página a partir del puntaje como autoridad de las páginas a las que apunta. Esto se puede reducir a una operación matricial.

\textbf{In-Deg}, El tercer método, el más intuitivo, se incluye principalmente como control. In-Deg consiste simplemente en contar los links que apuntan a cada página.\\

\newpage

\section{Desarrollo}
\label{sec:desarrollo}
%Deben explicarse los métodos numéricos que utilizaron y su aplicación al problema concreto involucrado en el trabajo práctico. Se deben mencionar los pasos que siguieron para implementar los algoritmos, las dificultades que fueron encontrando y la descripción de cómo las fueron resolviendo. Explicar también cómo fueron planteadas y realizadas las mediciones experimentales. Los ensayos fallidos, hipótesis y conjeturas equivocadas, experimentos y métodos malogrados deben figurar en esta sección, con una breve explicación de los motivos de estas fallas (en caso de ser conocidas).

%DEBERIAMOS PONER EN UNA PARTE DE METODOS MAL LOGRADOS, Q NO APROVECHABAMOS LA MATRIZ ESP PARA EL PRODUCTO NO?
%LO DE PASOS PARA IMPLEMENTAR LOS ALGORITMOS, TENDRIAMOS Q PONER Q PRIMERO HICIMOS EL MATRIZEsp Y LO DE CARGAR DATOS Snap (y dsp no me acuerdo la implementacion de page rank y hits al principio).
%Dificultades: Cargar de archivos con distintos renglones?, Leer los toronto (Problemas en el pageR seguro hubo, pero no me acuerdo.)
%Los ensayos fallidos, hipótesis y conjeturas equivocadas, experimentos y métodos malogrados??

\subsection{Criterio de almacenamiento}
Debimos primero encontrar una forma de almacenar los datos de los links entre p\'{a}ginas provistos (independientemente del formato en que fueron dados). Estos se guardan en la matriz $W$, ya mencionada.

Entre las 3 opciones sugeridas en el enunciado se opt\'{o} finalmente por el almacenamiento en forma de Compressed Sparse Column (CSC)\footnote{Una matriz esparsa que se recorre por columnas.} porque notamos que esto facilita aquellas operaciones que pueden ser necesarias para los algoritmos de mayor complejidad temporal (Pagerank y HITS) como:

\begin{itemize}

\item La suma de columnas (ya que se trata de sumar una secci\'{o}n de elementos contiguos del array en el que se guardan los valores de la matriz distintos de 0).

\item La divisi\'{o}n de una columna por una constante, debido a que al momento de definir en pagerank la probabilidad de salir de una p\'{a}gina a otra, debemos dividir las columnas de forma que sus elementos (las probabilidades de salir hacia alguna p\'{a}gina) sumen 1.

\item El producto de una matriz transpuesta por un vector, ya que puede recorrer una sección del vector de valores y multiplicarlos por una posición del vector. Para el producto de la matriz sin transponer el algoritmo es más complejo pero igual de eficiente: alcanza con recorrer los valores e ir acumulando en las distintas posiciones del vector resultado los valores parciales que se obtienen.

\end{itemize}

Además, tiene una eficiencia espacial \'{o}ptima para el almacenamiento de matrices que cuentan con muchos ceros (como es el caso de las matrices con las que trabajamos). M\'{a}s espec\'{i}ficamente : la memoria usada es $2 \times  O(m)+O(n)$.\footnote{$m$ es la cantidad de elementos diferentes de 0 y $n$ las filas de la matriz.} \\ 

En comparaci\'{o}n con las otras dos opciones de almacenamiento:

\begin{description}


\item[Dictionary of Keys] cuenta con s\'{o}lo la \'{u}ltima ventaja del CSC, que permite obviar a las posiciones en las que se guardan ceros, pero no es capaz de mejorar en ninguna manera la eficiencia de las operaciones matriciales, que se efect\'{u}an en los m\'{e}todos de ranking m\'{a}s complicados.

\item[Compressed Sparse Row (CSR)] cuenta con la \'{u}ltima ventaja del CSC tambi\'{e}n, pero no es capaz de operar con columnas de manera tan eficiente como el CSC.\\
Es necesario mencionar, sin embargo, que es una estructura m\'{a}s eficiente para realizar las operaciones que el \textit{In-Deg} requiere (por filas) por razones obvias, pero optamos por la \textit{Compressed Sparse Column} debido a que parece mejor optimizar las operaciones por columnas que intervienen en los algorítmos mas complejos (es posible modificar, sin embargo, las operaciones entre columnas para que se puedan realizar con una eficiencia análoga a las efectuadas por el CSC).
 
\end{description}

\newpage

\subsection{PageRank}

El algoritmo PageRank asigna un puntaje de importancia a cada página entre 0 y 1. Este no depende únicamente de la cantidad de links entrantes, sino que pondera cada link según el puntaje de la página al que pertenece. Sea $x$ el vector en $\mathbb{R}^n$ donde $x_j$ es el puntaje de la página $j$ y $n_j$ el $grado$ de $j$, es decir, la cantidad de links salientes, entonces se quiere ver que:
\begin{equation}
x_k = \sum_{j=1,w_{kj}=1}^{n} \frac{x_j}{n_j},~~~~k = 1,\dots,n.
\end{equation}

Supongamos que se tienen cuatro páginas para hacer el ranking (1, 2, 3, 4) y que están conectadas de esta forma:

$1 \rightarrow 2$

$1 \rightarrow 3$

$2 \rightarrow 3$

$3 \rightarrow 2$

$3 \rightarrow 4$

Entonces el problema se reduciría a buscar la solución a este sistema de ecuaciones:
\begin{center}
$\begin{bmatrix} 0&0&0&0\\\frac{1}{2}&0&\frac{1}{2}&0\\\frac{1}{2}&1&0&0\\0&0&\frac{1}{2}&0\end{bmatrix} \cdot \begin{bmatrix} x_1\\x_2\\x_3\\x_4 \end{bmatrix} = \begin{bmatrix} x_1\\x_2\\x_3\\x_4 \end{bmatrix}$
\end{center}
Llamamos $P$ a la matriz, que se construye dividiendo cada columna de $W$ por su suma. Se puede ver que la solución de este sistema de ecuaciónes es un vector $x$ tal que $Px=x$.

Se define un autovalor $\lambda$ de una matriz $A$ a un valor tal que $Ax=\lambda x$, y a $x$ como su autovector. En este caso, entonces, el problema se reduce a buscar un autovector para el autovalor 1. Dado que este autovector representa probabilidades, debe tener norma 1 igual a 1.

El método de la potencia permite encontrar el autovalor de mayor módulo de una matriz. Para poder aplicarlo en este caso, deben cumplirse las siguientes condiciones \footnote {Propuestas en \textit{Bryan, Leise - 2006 - The Linear Algebra behind Google $\ast$} y \textit{Kamvar, Haveliwala - 2003 - Extrapolation methods for accelerating PageRank computations}}:
\begin{itemize}
\item $P$ tiene un autovector asociado al autovalor 1
\item Los demás autovalores de la matriz cumplen $1 = \lambda_1 > |\lambda_2| \geq \dots \geq |\lambda_n|$ 
\item La dimensión del autoespacio asociado al autovalor $\lambda_1$ es 1: esto garantiza la unicidad del autovector de norma 1 igual a 1 asociado al autovalor 1.
\end{itemize}

Si la matriz $P$ es estocástica por columnas, se puede ver que se cumple las primeras dos condiciones:
Sea A una matriz en $\mathbb{R}^{n \times n}$ estocástica por columnas y sea $e$ un vector en $R^n$ tal que $e_i=1$ $\forall i$,
sabemos que $A$ y su transpuesta $A^T$ tienen los mismos autovalores. Como A es estocástica por columnas, se puede ver que $A^T e = e$, por lo que 1 es un autovalor de $A^T$ y, por tanto, de $A$. Además al estar dividida cada columna por su norma 1, la matriz es efectivamente estocástica por columnas, salvo que haya columnas cuya suma es 0. Esto se da en el caso de los llamados "dangling nodes", páginas sin links salientes. 

Se puede interpretar la navegación en la Web de un $navegante aleatorio$ como un proceso de Markov, a $P$ como su matriz de transición y a la componente $x_j$ del vector soluci\'on de norma 1 del sistema $Px = x$ como la proporci\'on del tiempo que el navegante pasa en la p\'agina $j$ Entonces, se puede suponer que al encontrarse en una página sin links salientes ($dangling node$), irá a cualquiera con probabilidad $1/n$. Se define entonces la matriz $P_1$, en la que se asigna 1/n, entonces, a cada fila de cada columna que representa un dangling node. En el ejemplo anterior:
\begin{center}
$P_1= \begin{bmatrix} 0&0&0&\frac{1}{6}\\\frac{1}{2}&0&\frac{1}{2}&\frac{1}{6}\\\frac{1}{2}&1&0&\frac{1}{6}\\0&0&\frac{1}{2}&\frac{1}{6} \end{bmatrix} $
\end{center}

Sin embargo, la última condición no se cumple necesariamente. Si se tienen dos subredes aisladas, entonces la dimensión del autoespacio será 2. Por ejemplo, sean las páginas (1,2,3,4), con los siguientes links: 

$1 \rightarrow 2$

$2 \rightarrow 1$

$3 \rightarrow 4$

$4 \rightarrow 3$

Su matriz de conectividad es:
\begin{center}
$W=\begin{bmatrix} 0&1&0&0\\1&0&0&0\\ 0&0&0&1\\0&0&1&0\end{bmatrix}$
\end{center}
Se puede ver que existen dos vectores diferentes de norma 1 tales que $Wx=x$, $x=\left( \frac{1}{2}, \frac{1}{2},0 ,0\right)$ y $x=\left(0,0,\frac{1}{2}, \frac{1}{2} \right)$ 

Para evitar esto, se agrega un parámetro $c>0$, donde $1-c$ representa la probabilidad de que un navegante aleatorio vaya a una página cualquiera. Se define entonces $P_2$ tal que
${P_2}_{ij} = c{P_1}_{ij} + \frac{1-c}{n}  \forall i$

$P_2$ cumple las tres condiciones, luego se puede buscar la solución de $P_2x=x$ con el método de la potencia. Sin embargo, no es esparsa; de hecho, no tiene ceros.

En el último ejemplo, con c=$4/5$:
\begin{center}
$W=\begin{bmatrix} \frac{1}{20}&\frac{17}{20}&\frac{1}{20}&\frac{1}{20}\\\frac{17}{20}&\frac{1}{20}&\frac{1}{20}&\frac{1}{20}\\ \frac{1}{20}&\frac{1}{20}&\frac{1}{20}&\frac{17}{20}\\\frac{1}{20}&\frac{1}{20}&\frac{17}{20}&\frac{1}{20}\end{bmatrix}$
\end{center}

Y el PageRank resultante sería: \\

\indent 1: 0,25 \\
\indent 2: 0,25 \\
\indent 3: 0,25 \\
\indent 4: 0,25 \\

\subsubsection{M\'{e}todo de la Potencia}

Este método permite encontrar al autovalor de mayor módulo de una matriz.

Sea $A \in \mathbb{R}^{n \times n}$, $\lambda_1 ... \lambda_n$ sus autovalores tales que $|\lambda_1|>|\lambda_2| \geq ... \geq |\lambda_n|$, $v_1$ ... $v_n$ una base de autovectores
%Si quieren dar vueltas, se puede demostrar que esto existe
y $x$ un vector cualquiera. $x$ se puede escribir como combinación lineal de los autovectores de $A$:
\begin{eqnarray*}
x & = &\sum_{j=1}^{n} \alpha_j v_j\\
Ax & = &\sum_{j=1}^{n} \alpha_j Av_j
\end{eqnarray*}

Como $v_j$ es autovector de $A$, entonces:
\begin{eqnarray*}
Ax&=&\sum_{j=1}^{n} \alpha_j \lambda_j v_j\\
A^2x&=&\sum_{j=1}^{n} \alpha_j \lambda_j Av_j\\
A^2x&=&\sum_{j=1}^{n} \alpha_j \lambda^2_j v_j
\end{eqnarray*}

\begin{eqnarray*}
&.&\\
&.&\\
&.&\\
A^kx&=&\lambda_1^k \cdot (\sum_{j=1}^{n} \frac {\lambda_j}{\lambda_1}^k v_j)\\
A^kx&=&\lambda_1^k \cdot (\alpha_1 v_1 \sum_{j=2}^{n} \frac {\lambda_j}{\lambda_1}^k v_j)\\
\frac{||A^kx||}{||A^{k-1x||}}&=&\frac{||\lambda_1^k \cdot (\alpha_1 v_1 \sum_{j=2}^{n} \frac {\lambda_j}{\lambda_1}^k v_j)||}{||\lambda_1^{k-1} \cdot (\alpha_1 v_1 \sum_{j=2}^{n} \frac {\lambda_j}{\lambda_1}^{k-1} v_j)||}\\
\end{eqnarray*}
cuando $k \rightarrow \infty$, $\frac{\lambda_j}{\lambda_1} \rightarrow 0$ porque $\lambda_1>\lambda_j$ $\forall j$, luego 
\begin{eqnarray*}
\frac{||A^kx||}{||A^{k-1}x||} \rightarrow \frac{||\lambda_1^k \cdot (\alpha_1 v_1) ||}{||\lambda_1^{k-1} \cdot (\alpha_1 v_1)||} = \lambda_1
\end{eqnarray*}

$A^kx=((...((AA)A)...A)A)x=A(A(A(...(A(Ax)))$\\
%Faltaria explicar porq A a la k por x te da el aoutovector asociado al autovalor 1
Es decir que se puede implementar un algoritmo que calcule $Ax$ y asigne el resultado a $x$, siendo este el autovector buscado.

Para el contexto particular del PageRank, utilizamos el algoritmo 1 propuesto por Kamvar\footnote{Kamvar, Haveliwala - 2003 - Extrapolation methods for accelerating PageRank computations}, que permite calcular $P_2 x$ a partir de $Px$. Esto permite ahorrar tiempo y memoria, ya que $P$ es una matriz esparsa. El algoritmo consiste de tres pasos:
\begin{eqnarray*}
y&=&c(Px)\\
w&=&||x||_1-||y||_1\\
y&=&wv\\
\end{eqnarray*}

Donde $v$ es un vector en $ \textbf{R}^n$ tal que $v_i=1/n \forall i$

Es decir, $y_i=c(Px)_i+(1-c||Px||_1)/n$ \\
%||Px||&=&\sum_j (Px_j)\\
%(Px_j)&=&\sum_i p_{ji} x_i

%||Px||=suma_j (Px_j)

%||Px||=suma_j (sum_i p_{ji} x_i)

%||Px||=suma_i x_i·(sum_j p_{ji})

%sum_j p_{ji}=0:

%||Px||=0 => (Px)_i=0 \forall i

%$y_i=1/n$ \\

%sum_j p_{ji}>0:

\subsection{Hits}

La idea detr\'as de este m\'etodo es, como se ha explicado brevemente antes, realizar un an\'{a}lisis de la red y sus p\'aginas vi\'endolas como hubs y autoridades, nociones que son definidas por las relaciones (links) que tienen con el resto de la red.\\

Dada una b\'{u}squeda, el objetivo es retornar un subconjunto de páginas relevantes.
Se asigna entonces sobre las p\'aginas el rol de $autoridad$ en el tema que se busca. Su valor como $autoridad$ est\'a definido por el valor de $hub$ de las p\'{a}ginas que apuntan a ella mientras que el valor o peso de $hub$ est\'a definido, a su vez, por el valor de $autoridad$ de las p\'{a}ginas a las que apunta.\\

Recordemos que los links están almacenados en la matriz de conectividad $W^t$ donde $w^t_{ij} = 1$ si existe un link de la p\'agina $j$ a la p\'agina $i$,  $w^t_{ij} = 0$ en caso contrario. Para cada p\'agina $i \in Web$ se define como peso de autoridad $x_i$ y como peso de hub $y_i$.
A partir de esto, podemos definir los vectores $x,y \in \mathbb{R}^n$ como los vectores de pesos de autoridad y hubs, y se puede suponer que se encuentran normalizados.
Evidentemente, se desprende de esto que las p\'{a}ginas con mayores $x_i$ o $y_i$ son consideradas las mejores $autoridades$ o $hubs$.

Se puede expresar la transferencia de pesos de hubs y autoridad, respectivamente, de la siguiente manera:

\begin{description}

\item[Peso de Autoridad] $x_j = \sum_{i} y_i$

\item[Peso de Hub] $y_i = \sum_{j} x_j$

\end{description}

Consecuentemente, si expresamos con operaciones matriciales los vectores de hub y autoridad:
\begin{eqnarray*}
x &=& Wy \\
y &=& W^tx
\end{eqnarray*}

Se aplica luego el paso de normalizaci\'{o}n $y \leftarrow \frac{y}{||y||}$ y se vuelve a comenzar hasta llegar a la convergencia.
Para la primera iteraci\'{o}n es posible empezar con un $y_0$ inicial cualquiera, y el m\'{e}todo converger\'{a} si se cumplen las siguientes condiciones:

\begin{itemize}
\item $W$ tiene un autovalor $\lambda_1$ tal que $|\lambda_1| > |\lambda_2| \geq \dots \geq |\lambda_n|$ 
\item $y_0$ no es ortogonal al autovector asociado a $\lambda_1$
\end{itemize}

% Justificación: (creo que esta es la parte opcional).

%One easily verifies that the I and O operations can be written $x \leftarrow A^T y$ and $y \leftarrow Ax$, respectively. Thus, $x_k$ is the unit vector in the direction of $(A^T A)^{k-1}z$, and $y_k$ is the unit vector in the direction of $(AA^T)^kz$.

%Now, a standard result of linear algebra states that if $M$ is a symmetric $n \times n$ matrix, and $v$ is a vector not orthogonal to the principal eigenvector $\lambda 1 (M)$, then the unit vector in the direction of $M^k v$ converges to $\lambda_1 (M)$ as $k$ increases without bound.

%Also (as a corollary), if $M$ has only nonnegative entries, then the principal eigenvector of $M$ has only nonnegative entries.

%Consequently, $z$ is not orthogonal to $\lambda_1 (A A^T)$, and hence the sequence $\{y^k\}$ converges to a limit $y^*$. Similarly, one can show that if $\lambda_1 ( A^T A) \neq 0$ (as dictated by Assumption ()), then $A^T z$ is not orthogonal to $\lambda 1 ( A^T A)$. It follows that the sequence $\{x^k\}$ converges to a limit $x^*$.

\subsection{In-Deg}

Este m\'etodo es considerablemente m\'{a}s simple y sigue una idea m\'{a}s intuitiva de ranking de p\'aginas sin llegar a entrar en complejizaciones para tener en cuenta distintos casos, lo que consideramos que lleva a unos resultados menos fiables. Esto se debería notar especialmente cuando son contrastados con aquellos que obtenemos de analizar redes con m\'etodos que dan m\'as peso a detalles que potencialmente cambiarían los resultados (por ejemplo: una página apuntada por pocas páginas de gran valor caería muy por debajo en el ranking, mientras que probablemente PageRank y HITS le asignarían una posición más alta).

La idea b\'asica es definir el valor de un p\'agina seg\'un la cantidad de p\'aginas que la apuntan.

En otras palabras: el método consiste en sumar los links a cada página y, teniendo en cuenta que ya contamos con la matriz $W$, este método se reduce a sumar la fila correspondiente a cada página y obtener as\'i un vector que nos muestra el valor de cada una (la i\'esima posición corresponde a la iésima página, según su posición asignada en la matriz $W$). 


\subsection{Experimentos realizados}
Para los dos primeros métodos, se realizaron experimentos para ver la convergencia de la diferencia en cada iteración de la norma del autovector buscado. En PageRank, se utilizo la norma 1 del autovector $x$. En el caso de HITS, se analizó los vectores $x$ e $y$ tomando norma 2.
Luego, se comparó el tiempo de ejecución de cada uno utilizando la librería time.h de C++. No se tomó en cuenta el tiempo que toma cargar la matriz. Como una iteración de HITS hace dos productos de una matriz por un vector, mientras que PageRank hace uno, y que esta operación es la que domina el costo temporal de la ejecución, se esperaba que el tiempo de ejecución de HITS fuera alrededor del doble del de PageRank, independientemente del valor de $c$ usado. Este influye en el tiempo de ejecución\footnote{Kamvar, Haveliwala - 2003 - Extrapolation methods for accelerating PageRank computations} pero no esperábamos que modificara sustancialmente esta relación.

Dado que indexamos la matriz desde 1, reemplazamos cada página numerada como 0 por $n+1$ para poder realizar los tests.

Las instancias de test usadas fueron las siguientes:\\ \\
\begin{tabular}{c|r |r |r}
$\sigma$ & nodos & links & $\frac{link}{nodo}$\\
\hline
Movie & 5.757 & 24.451 & 4,24\\
\hline
Censorship & 2.947 & 9.555 & 3,24\\
\hline
Genetic & 3.468 & 12.689 & 3,65 \\
\hline
NotreDame & 325.729 & 1.497.134 & 4,59 \\
\hline
Stanford & 281.903 & 2.312.497 & 8,20 \\
\end{tabular}

Los valores de $c$ usados para el pageRank fueron los siguientes:
\begin{itemize}
\item 0.5
\item 0.7
\item 0.85
\item 0.95
\end{itemize}
Mientras que la tolerancia para asumir convergencia la fijamos en $10^{-4}$

A continuación se experimentó con una muestra de links entre varias páginas obtenida mediante el script provisto por la cátedra. De esta forma esperábamos analizar los resultados obtenidos en función de qué páginas se esperaban con mayor ranking.
%Experimentos:
%Ver la covergencia de la norma 1 de Pagerank
%Ver la convergencia de la norma ¿2? de HITS
%Comparar tiempos
%Comparar calidades

\newpage
\section{Resultados}
\label{sec:res}
%Deben incluir los resultados de los experimentos, utilizando el formato más adecuados para su presentación. Deberán especificar claramente a qué experiencia corresponde cada resultado. No se incluirán aquí corridas de máquina.
\subsection{Convergencia de Norma Manhattan de Pagerank}

Estos son los resultados obtenidos de convergencia de la norma para los distintos tests.

En los gráficos de PageRank se compara la evolución de $||x^k-x^{k-1}||_1$ para distintos valores de $c$.\\

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.25]{img/Normas-Page-Censor.png} 
\includegraphics[scale=0.25]{img/Normas-Page-Genetic.png} 
\includegraphics[scale=0.25]{img/Normas-Page-Movie.png}

\caption{Normas (PageRank): escala logarítmica}
\end{figure}

\begin{center}
\begin{tabular}{r|c|c|c}
&  \textbf{Censorship} & \textbf{Genetic} & \textbf{Movies} \\ 
\hline
C & Iteración de Llegada & Iteración de Llegada & Iteración de Llegada\\
\hline
0.5 & 6 & 7 & 7\\
\hline
0.7 & 9 & 12 & 10\\
\hline
0.85 & 19 & 25 & 19\\
\hline
0.95 & 58 & 77 & 58
\end{tabular}
\end{center}


\begin{figure}[htbp]
\centering
\includegraphics[scale=0.35]{img/Normas-Page-Notre.png}
\includegraphics[scale=0.35]{img/Norma-Page-Stanford.png}
\caption{Normas (PageRank): escala logarítmica}
\end{figure}

\begin{center}
\begin{tabular}{r|c|c}
 & \textbf{Notredame} & \textbf{Stanford}\\
\hline
C & Iteración de Llegada & Iteración de Llegada \\
\hline
0.5 & 8 & 8\\
 \hline
0.7 & 9 & 9\\
\hline
0.85 & 16 & 18\\
\hline
0.95 & 46 & 55
\end{tabular}
\end{center}

%\caption{Normas (PageRank): escala logarítmica}
%\label{fig:graficos_cropflip}

\newpage

En los gráficos de HITS se compara la evolución de $||x^{(k)}-x_{(k-1)}||_2$ con la de $||y^{(k)}-y^{(k-1)}||_2$.\\
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.25]{img/Normas-HITS-Censor.png}
\includegraphics[scale=0.25]{img/Normas-HITS-Genetic.png} 
\includegraphics[scale=0.25]{img/Normas-HITS-Movie.png}

\caption{Normas (HITS): escala logarítmica}
\end{figure}

\begin{center}

\begin{tabular}{r|c|c|c}
&  \textbf{Censorship} & \textbf{Genetic} & \textbf{Movies} \\ 
\hline
 Iteración de Llegada & 29 & 14 & 15\\

\end{tabular}


\end{center}


\begin{figure}[htbp]
\centering
\includegraphics[scale=0.385]{img/Normas-HITS-Notre.png}
\includegraphics[scale=0.385]{img/Norma-HITS-Stanford.png}
\caption{Normas(HITS): escala logarítmica}
\label{fig:graficos_cropflip}
\end{figure}

\begin{center}

\begin{tabular}{r|c|c}
&  \textbf{NotreDame} & \textbf{Stanford}\\ 
\hline
 Iteración de Llegada & 30 & 18\\
\end{tabular}

\end{center}

Estos son los resultados de tiempo de cómputo obtenidos, en ciclos de clock:

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.25]{img/Tiempos-Censor.png}
\includegraphics[scale=0.25]{img/Tiempos-Genetic.png}
\includegraphics[scale=0.25]{img/Tiempos-Movie.png}
\caption{Tiempo de cómputo}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.385]{img/Tiempos-Notre.png}
\includegraphics[scale=0.385]{img/Tiempo-Stanford.png}
\caption{Tiempo de cómputo}
\end{figure}

\newpage
\begin{center}
\begin{tabular}{l|c|c|c|c|c}
\hline
& Movie & Censorship & Genetic & NotreDame & Stanford\\
\hline
Ciclos por Iteración PageRank & 1896.1 & 822 & 882.2 & 103619.1 & 115099.8 \\
\hline
Ciclos por Iteración HITS & 3857.8 & 1213.4 & 2149.5 & 131540.1 & 206502.1
\end{tabular}
\end{center}

\subsection{Puntajes obtenidos}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.25]{img/casorealPage.png}
\includegraphics[scale=0.25]{img/casorealHits.png}
\includegraphics[scale=0.25]{img/casorealIn-Deg.png}
\caption{Puntajes obtenidos con los distintos métodos por distintas páginas}
\end{figure}

\begin{tabular}{r|l}
1 & www.google.com\\
2 & www.clarin.com\\
3 & www.clarin.com/deportes\\
4 & www.lanacion.com.ar\\
5 & www.infobae.com\\
6 & canchallena.lanacion.com.ar\\
7 & www.rollingstone.com.ar\\
8 & www.ole.com.ar \\
9 & www.clasificados.clarin.com\\
10 & www.mamapuntocero.com.ar\\
11 & www.ciudad.com.ar\\
12 & www.zonaprop.com.ar\\
13 & www.pagina12.com.ar\\
14 & www.yahoo.com\\
\end{tabular}

\subsection{Redes pequeñas}
Consideramos los siguientes casos:
\begin{itemize}
\item Una red en la que una página apunta a todas
\item Una red en la que todas las páginas apuntan a una
\item Una red en la que casi todas las páginas se apuntan entre sí
\item Una red con muy pocos links
\end{itemize}
Para observar el comportamiento de PageRank y HITS. Obtuvimos estos resultados:

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.385]{img/todosal2.png}
\includegraphics[scale=0.385]{img/todosal2out.png}
\caption{Puntajes obtenidos para una red en la que todas las páginas apuntan a la nº 2}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.385]{img/eldosatodos.png}
\includegraphics[scale=0.385]{img/eldosatodosH.png}
\caption{Puntajes obtenidos para una red en la que la pagina nº 2 apunta a todas}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.385]{img/PocosLinksout.png}
\includegraphics[scale=0.385]{img/PocosLinksoutH.png}
\caption{Puntajes obtenidos para una red con pocos links}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.385]{img/muchoslinksout.png}
\includegraphics[scale=0.385]{img/muchoslinksoutH.png}
\caption{Puntajes obtenidos para una red con muchos links}
\end{figure}

\newpage
\section{Discusi\'{o}n}
\label{sec:discusion}
%Se incluirá aquí un análisis de los resultados obtenidos en la sección anterior (se analizará su validez, coherencia, etc.). Deben analizarse como mínimo los items pedidos en el enunciado. No es aceptable decir que “los resultados fueron los esperados”, sin hacer clara referencia a la teoría la cual se ajustan. Además, se deben mencionar los resultados interesantes y los casos “patológicos” encontrados.

En PageRank, las normas convergen a 0 en forma exponencial luego de las primeras iteraciones. Cuanto mayor sea el valor de $c$, más tarda en alcanzarse el criterio de parada. Como ya se dijo, en un caso extremo, con $c=1$, la norma podría no converger. En HITS, las normas también convergen a 0 en forma exponencial.\\

En todos los casos analizados se obtuvo que el tiempo de cómputo de PageRank aumentó exponencialmente con el valor de $c$.\\

El algoritmo HITS resultó más lento que PageRank para todos los valores de $c \leq 0,85$ que utilizamos, en todas las redes para las que medimos los tiempos. Sin embargo, consideramos que esta diferencia se debe principalmente a que el algoritmo HITS debe realizar dos productos entre una matriz y un vector, mientras que PageRank solo realiza una. Se puede comprobar que esta diferencia es proporcional al tamaño de la matriz que representa a la red. Es decir, la diferencia no llega a ser de órdenes de complejidad.\\

Además, se espera que si se eligen valores de $c$ mayores a un valor, el tiempo de ejecución de Pagerank siempre supere al de HITS.\\

En los tests de mayor tamaño se puede ver que la cantidad de iteraciones necesarias para alcanzar convergencia es igual que en los tests menores o incluso menor. Si tienen mayor tiempo de cómputo, es porque la multiplicación entre la matriz y el vector que es necesaria en cada iteración tiene una mayor cantidad de elementos. Esto se puede ver en el tiempo de iteración. Sin embargo, dada la proporción de links por páginas, esto tampoco influye demasiado.\\

El valor de $c$, si bien influye en el puntaje, no lo varía demasiado. Un valor más alto de $c$ implica darle una mayor importancia a los links que relacionan las páginas, resultando en un ranking más representativo de la estructura de la red. Sin embargo, no necesariamente será mejor como modelo para el comportamiento de un navegante aleatorio en la práctica.\\

Teniendo en cuenta que la cantidad de iteraciones, y por lo tanto el tiempo de ejecución, escalan exponencialmente con este parámetro, es conveniente elegir un valor no demasiado alto. \\

Al hacer un análisis cualitativo de los puntajes obtenidos se pueden obvervar que el puntaje no es el esperado. Por ejemplo, se esperaba que Google obtuviera el mayor puntaje de hub, cosa que no ocurrió.\\

Esto se debe a que la porción de la red elegida no es representativa. Se ve que lanacion.com obtuvo el mayor puntaje de PageRank, pero hay que tener en cuenta que en la lista hay sitios que pertenecen al mismo dominio.\\

\newpage

\section{Conclusiones}
\label{sec:conclusiones}

%Esta sección debe contener las conclusiones generales del trabajo. Se deben mencionar las relaciones de la discusión sobre las que se tiene certeza, junto con comentarios y observaciones generales aplicables a todo el proceso. Mencionar también posibles extensiones a los métodos, experimentos que hayan quedado pendientes, etc.

En el paper escrito por Brin y Page \footnote{Brin, Page - 1998 - The anatomy of a large-scale hypertextual Web search engine} se recomienda tomar un $c$ (probabilidad de un navegante aleatorio de seguir un link de la página en la que está) de 0.85.\\ \\%O sea, hacer experimentos 

Consideramos sin embargo que, de contar con los recursos necesarios para realizar mediciones de la magnitud necesaria para obtener resultados representativos, la mejor manera de definir un valor de $c$ fiable en la mayor cantidad de casos posibles sería acercarnos a la probabilidad real con la que el navegante aleatorio $salta$ a otra página sin seguir un link, para esto haría falta hacer experimentos con navegantes reales y realizar un promedio de los $c$ que obtuvimos de estos navegantes; es decir que el mejor $c$ a tomar sería la esperanza de los $c$ obtenidos en estas mediciones.\\

El hecho de que la norma converja exponencialmente permite que la cantidad de iteraciones sea lo suficientemente acotada, teniendo en cuenta que cada iteración es una multiplicación de una matriz por un vector que puede tener varios millones de posiciones.


Luego de haber contrastado estos distintos métodos de ranking podemos decir con seguridad que:\\


\textbf{In-Deg} es completamente descartable como método para decdidir la relevancia de una página, se limita a realizar el análisis más previsible y propenso a caer en errores.
Buscadores puramente textuales que siguieron este método, como AltaVista, fueron reemplazados eventualmente por otros más útiles, si bien fueron un paso necesario para la creación de buscadores mejores.\\\\

\textbf{HITS} es mucho más efectivo y confiable ya que realiza un análisis más inteligente de la red que le es proporcionada, confiriéndole distintos roles a las páginas y contrastándolas a partir de las relaciones entre estas páginas según el rol que estén cumpliendo.\\
Hemos comprobado que la cantidad de links a una página no son relevantes por sí solos para considerarla una buena autoridad sino que lo más importante es la relevancia como hubs de aquellas páginas que la apuntan, esto revela un análisis mucho más profundo que el de \textit{Indeg}.\\
Por la naturaleza del análisis que realiza es, sin embargo, más susceptible de caer en engaños tales como que páginas acuerden apuntarse entre sí para ganar peso en el ranking que \textit{HITS} les asigne, mientras que con \textit{PageRank} este artilugio no daría frutos debido a que cuantos más links salientes una página tenga, menos valor le estará transfiriendo con cada link a las páginas apuntadas.\\\\

\textbf{PageRank} es el método que consideramos mejor, superando tanto a \textit{HITS} como a \textit{Indeg} debido a que realiza un análisis profundo de las relaciones entre páginas, distinto de aquel proporcionado por \textit{HITS}. En ciertos casos parece que el procedimiento es opuesto. Por ejemplo, una página que apunta a todas le da poco valor a cada una en PageRank, pero al apuntar a muchas autoridades es un buen hub según HITS, luego le da valor a los que apunta. Un inconveniente que tiene este algoritmo es que trata una pagina con links a toda la web igual que un dangling node, mientras que HITS le da a un gran puntaje como hub.
\\\\

Por ultimo, si el cliente quiere tener un mejor PageRank, debe conseguir que lo apunten páginas que tengan la mejor proporción entre PageRank y cantidad de links salientes, ya que el puntaje se distribuye entre todas las páginas. En cambio, si quiere conseguir un mejor puntaje como autoridad en HITS, debe buscar que la mayor cantidad posible de las mejores páginas lo apunten. Si desea tener un mejor puntaje de hub tendría que apuntar a las páginas con mayor autoridad en HITS.\\
Podemos finalmente concluir que la mejor estrategia a sugerir a clientes sería darle prioridad al ranking proporcionado por Pagerank, está además la prueba de la efectividad del Pagerank en el éxito que tuvo Google al usarlo. 
Sugeriríamos entonces apoyarse en el orden de resultados que proporcione este buscador (que es además el más usado) para decidir en qué páginas comprar espacio de publicidad.

\newpage

\section{Apéndice A}
\label{sec:ApA}

\input{tp2}

\newpage
%En el apéndice B se incluirán los códigos fuente de las funciones relevantes desde el punto de vista numérico. Resultados que valga la pena mencionar en el trabajo pero que sean demasiado específicos para aparecer en el cuerpo principal del trabajo podrán mencionarse en sucesivos apéndicesaerotulados con las letras mayúsculas del alfabeto romano. Por ejemplo: [la demostración de una propiedad que aplican para optimizar el algoritmo que programaron para resolver un problema].

\section{Referencias}
\label{sec:ref}

Apuntes de clase

R. Burden y J.D.Faires, Análisis numérico, International Thomson Editors, 1998

http://www.cs.toronto.edu/~tsap/experiments/datasets/.

Stanford large network dataset collection. http://snap.stanford.edu/data/#web.

Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1-7):107–117, April 1998.

Kurt Bryan and Tanya Leise. The linear algebra behind google. SIAM Review, 48(3):569–581, 2006.

Sepandar D. Kamvar, Taher H. Haveliwala, Christopher D. Manning, and Gene H. Golub. Extrapolation methods for accelerating pagerank computations. In Proceedings of the 12th international conference on World Wide Web, WWW ’03, pages 261–270, New York, NY, USA, 2003. ACM.

Jon M. Kleinberg. Authoritative sources in a hyperlinked environment. 46(5):604–632, September 1999.


\end{document}